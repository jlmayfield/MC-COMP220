\documentclass[nobib]{tufte-handout}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{listings,color}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{
  frame=tb,
  language=C++,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=left,
  stepnumber=1,
  firstnumber=1,
  numbersep=5pt,
  numberfirstline=true,
  numberstyle=\tiny\color{black},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}


\title{COMP 220 \\ Lecture Notes 02 \\ Analyzing Simple Recursion}

\begin{document}
\maketitle

\begin{abstract}
In COMP161 we mostly looked closely at the analysis of loops. Here we look at simple recursive functions and their analysis.
\end{abstract}

\section{Repetition is as Repetition Does}

The essential analysis task of analyzing a recursive function is the same as analyzing a loop because they both induce the same kind of property in out computations: repetition. Recursive functions do so by repeatedly calling themselves where loops manage it via the implicit repetition of C++ loop control structures. What this means is we must still look for three things:
\begin{enumerate}
  \item The amount of work carried out independently of the repetition.
  \item The amount of work carried out per repetition
  \item The frequency of repetition.
\end{enumerate}

\section{Recursive Linear Search}

In lecture notes 16 from COMP161\sidenote{\url{https://jlmayfield.github.io/MC-COMP161/Lecture_Notes/comp161-lectureNotes-16.pdf}} we looked at linear search implemented recursively. This implementation is shown again in figure~\ref{code:searchrec} and utilizes the two function pattern that is typical with our design methodology for developing recursive functions for C++ vectors. The second function utilizes index parameters \textit{fst} and \textit{lst} in order to manage the recursive decomposition of the vector's index range.

\begin{figure}[htpb!]
\begin{lstlisting}
int search(const std::vector<int>& data,int key){
	return search(data,0,data.size(),key);
}

int search(const std::vector<int>& data,int fst, int lst, int key){
	if( fst >= lst ){
		return -1;
	}
	else if( data[fst] == key ){
		return fst;
	}
	else{
		return search(data,fst+1,lst,key);
	}
}
\end{lstlisting}
\label{code:searchrec}
\caption{Search: Recursive Implementation}
\end{figure}

To start our analysis we start at the top with the search which takes the vector and key and searches the entire vector. This procedure carries out $O(1)$ operations to get the vector size and then makes a recursive call. This means that the complexity is $O(1) + O(f)$ where $O(f)$ is the complexity of the recursive helper. At this point we're pretty much done with this procedure.  It's complexity is exactly the complexity of the helper\sidenote{Do you see why?}, i.e. $O(f)$. The trick is not not get hung up on the unknown function $f$ just yet. Take a moment to come to a decisive conclusion about the analysis of the top level function that is independent of the details of the analysis of it's helper.

Now that we're more or less done thinking about the top level function, we can continue wit the helper. The worst case clearly occurs with the condition for the \textit{if} and \textit{else if} is false and we drop down to the recursive call in the \textit{else}.  When this happens then the procedure will perform exactly $4$ operations prior to making the recursive call\sidenote{$>=$, $\lbrack \rbrack$, $==$, and $+$}.

We now need to think about how recursion is carried out. The function gets called over and over until the base case is reached in which no recursive call is made. If $n$ recursive calls are made, then $n-1$ of those make a recursive call and $1$ is the terminating base case. Combine this with our previous analysis, we known that $4$ operations are done when the procedure makes a recursive call, but what about the final recursive call, the base case call?

The code clearly indicates that two situations can terminate the recursion. The first is covered by the \textit{if} and occurs when the the index range specified by \textit{fst} and \textit{lst} is empty. This case requires a single operations. The next situation is when for a non-empty index range, the key is found at \textit{fst}. This case requires three operations. Given that our concern is overall worst case complexity, we need to recognize that the terminating case that maximizes work is the first one because terminating there means we've searched an entire index range and not a part of a range.

We now known that one operation, the work of the terminating base case, is carried out outside the repetition and that four operations are carried out every time a recursive call is made. We're left the question of how many recursive calls are made? To determine this we can dig into the code logic. Every recursive call advances the value of \textit{fst} by one. Recursion terminates when \textit{fst}$>=$\textit{lst}. In short, we're dealing with counting logic just like our loops.

\begin{equation}
  \begin{array}{rcl}
  fst + k &>&= lst \\
  k &>& lst - fst
  \end{array}
\end{equation}

Now recall that $lst-fst+1$ is the number of integers in the range of $fst$ to $lst$ if we exclude last. So a simpler way of state what we found is that \textit{the recursion repeats once for each of the elements in the index range}.  When searching an entire vector, this is exactly the size of the vector. We can now find the exact work done by this function and it's complexity. For a vector of size $n$ we'll do $4n+1$ operations with a complexity of $O(n)$.

\section{Recurrence Relations}

Our analysis of liner search mostly relied on feeling our way through the code and recognizing a pattern familiar from iteration. There is a way of analyzing the complexity of a recursive function that is more general from the perspective that it relies less on recognizing the kind of counting that typifies iteration.

Before we determined the frequency of repetition we recognized two crucial cases to the worst case complexity for our function.
\begin{enumerate}
  \item The empty range base case that carries out a single operation
  \item The recursive case that carries out four operations plus the cost of recursively searching the remainder of the vector.
\end{enumerate}

We can express this knowledge of the complexity of search as a recursive function. Let $n$ be the size of the index range\sidenote{$n=$\textit{fst}-\textit{lst}} and $\mathcal{T}$ be the function that maps $n$ to the number of operations needed for the worst case search. Then equation~\ref{eq:searchRecRel} expresses $\mathcal{T}$ recursively!
\begin{equation}
  \begin{array}{rcl}
    \mathcal{T}(0) &=& 1 \\
    \mathcal{T}(n) &=& \mathcal{T}(n-1) + 4
  \end{array}
\label{eq:searchRecRel}
%\caption{Recurrence Relation for Linear Search}
\end{equation}

Equations like those shown in equation~\ref{eq:searchRecRel} are called \textsc{Recurrence Relations} and are incredibly useful in the analysis of algorithms. The problem is that they are \textit{open form solutions} to our problem. They do not provide us with an exact, calculable solution. What we really want is a \textit{closed form} solution where $\mathcal{T}$ is not on the right hand side of the equation. To do this we must solve the recurrence.

\subsection{Solving Simple Recurrences via Unrolling}

The recurrence relation shown in equation~\ref{eq:searchRecRel} is about as simple as they come. The easiest way to solve it is usually to just unroll the recursion and determine the underlying pattern.  As has become the norm, you're goal is to establish an equation for $\mathcal{T}(n)$ after $k$ levels of unrolling or recursion depth $k$. The initial equation gives us depth $1$ and we take it from there.
\begin{equation*}
  \begin{array}{rcl}
    \mathcal{T}(n) &=& \mathcal{T}(n-1) + 4 \\
    &=& (\mathcal{T}(n-2) + 4) + 4 = \mathcal{T}(n-2) + 2*4 \\
    &=& (\mathcal{T}(n-3) + 4) + 2*4 = \mathcal{T}(n-3) + 3*4 \\
    & & \ldots \\
    &=& (\mathcal{T}(n-k) + 4) + 4(k-1) = \mathcal{T}(n-k) + 4k \\
  \end{array}
\end{equation*}

To finish the process and solve the recurrence we must determine when the recurrence terminates, the max depth of the recursion. The base case occurs when $n=0$ so the recurrence terminates when $n-k=0$ or, solving for $k$, with a recursive depth of $k=n$. When this happens, then we substitute the base case for the known value of  $\mathcal{T}(0)$ on the right and find the closed form solution found in equation~\ref{eq:searchclosed}.

\begin{equation}
\mathcal{T}(n) = 4n+1
\label{eq:searchclosed}
\end{equation}

To be certain that we're correct we can do a quick proof by induction or at the very least check the core logic of the proof.  When $n$ is zero our closed form gives us $\mathcal{T}(0) = 1$. The base case holds. Now we assume everything works great for the first $n-1$ steps and verify that this implies the above equation.
\begin{equation*}
  \begin{array}{rcl}
    \mathcal{T}(n) &=& \mathcal{T}(n-1) + 4 \\
    &=& (4(n-1) + 1)  + 4 \\
    &=& 4n - 4 + 1 + 4 \\
    &=& 4n + 1
  \end{array}
\end{equation*}
Thus, by the principle of mathematical induction our closed form solution matches the open form recurrence relation.

\section{Simple Recurrences}

In lab we noticed that basic step counting loops will keep us firmly in the ballpark of linear time algorithms or worse. The same pattern holds true for these kind of simple recurrences. What if the base case has a fixed cost of $a$, each recursive call has a fixed cost of $w$, and each step steps towards the base case by some fixed amount $s$. Then we'd be dealing with a recurrence that fits the template shown in equation~\ref{eq:simpRecRel}.

\begin{equation}
  \begin{array}{rcl}
    \mathcal{T}(0) &=& a \\
    \mathcal{T}(n) &=& \mathcal{T}(n-s) + w
  \end{array}
\label{eq:simpRecRel}
\end{equation}

\begin{equation}
  \begin{array}{rcl}
    \mathcal{T}(n) &=& \mathcal{T}(n-s) + w \\
    &=& \mathcal{T}(n-2s) + 2w \\
    &=& \mathcal{T}(n-3s) + 3w \\
    && \ldots \\
    &=& \mathcal{T}(n-ks) + kw
  \end{array}
\label{eq:simpRecRel}
\end{equation}

We now solve for $k$ when $n-ks=0$ to get $k = \lceil \frac{n}{s} \rceil$ for a closed form of:
\begin{equation}
    \mathcal{T}(n) = \frac{nw}{s} + a
\end{equation}
For fixed base case cost $a$, step size $s$, and work per recursion $w$, this is linear time, $O(n)$.

\end{document}
